{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b4099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import odeint\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import time\n",
    "from scipy.optimize import basinhopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952d4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b39419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6bdcc00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1000.,    0.,    0.,    0., 1100.,    0.,    0.,    0., 1200.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e2340cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_a_timeseries = np.array([5, 6, 9, 12, 15, 19, 24, 30, 37, 46, 56, 68, 75, 90])\n",
    "patch_b_timeseries = np.array([5, 7, 12, 18, 26, 37, 53, 76, 107, 150, 180, 250, 400, 500])\n",
    "patch_c_timeseries = np.array([2, 2, 4, 5, 8, 11, 14, 19, 25, 33, 44, 58, 75, 98])\n",
    "\n",
    "home_patches = np.array([0,4,8])\n",
    "\n",
    "# X-axis (assuming time steps)\n",
    "time_steps = np.arange(len(patch_a_timeseries))\n",
    "\n",
    "def expand_array(short_array):\n",
    "    long_array = np.zeros(9)  # Create a zero array of the desired size\n",
    "\n",
    "    # Assign values at the correct positions\n",
    "    long_array[np.arange(3) * (3 + 1)] = short_array\n",
    "    return long_array\n",
    "\n",
    "\n",
    "\n",
    "all_station_timeseries = np.array([\n",
    "    patch_a_timeseries,\n",
    "    patch_b_timeseries,\n",
    "    patch_c_timeseries\n",
    "])\n",
    "\n",
    "\n",
    "pop_dict = {\n",
    "    \"Patch A\": 1000,\n",
    "    \"Patch B\": 1100,\n",
    "    \"Patch C\": 1200\n",
    "}\n",
    "\n",
    "short_pop_vector = np.array(list(pop_dict.values()))\n",
    "pop_vector = expand_array(short_pop_vector)\n",
    "\n",
    "station_list = [\"Patch A\",\"Patch B\",\"Patch C\"]\n",
    "\n",
    "p_matrix = np.array([\n",
    "    [0.836, 0.014, 0, 0.014, 0.071, 0.011, 0, 0.011, 0.043],\n",
    "    [0, 1.0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1.0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1.0, 0, 0, 0, 0, 0],\n",
    "    [0.071, 0.009, 0, 0.009, 0.816, 0.016, 0, 0.016, 0.064],\n",
    "    [0, 0, 0, 0, 0, 1.0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1.0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 1.0, 0],\n",
    "    [0.043, 0.005, 0, 0.005, 0.064, 0.027, 0, 0.027, 0.828]\n",
    "])\n",
    "\n",
    "sparse_p = csr_matrix(p_matrix)\n",
    "\n",
    "Nj = pop_vector @ p_matrix\n",
    "\n",
    "valid_patch_indices = np.array([0,1,3,4,5,7,8])\n",
    "\n",
    "station_borough_list = [\"Borough AB\",\"Borough AB\",\"Borough C\"]\n",
    "\n",
    "borough_list = [\"Borough AB\",\"Borough C\"]\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define function to process solution\n",
    "def process_solution(solution,timesteps):\n",
    "    # Grouping by station subpopulation\n",
    "    S_sol = solution[:9]\n",
    "    I_sol = solution[9:18]\n",
    "    I_sol_total = solution[18:]\n",
    "\n",
    "    return [S_sol,I_sol,I_sol_total]\n",
    "\n",
    "\n",
    "\n",
    "def run_model_London(params):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    S0,I0,I_total0,beta,gamma,timesteps = params\n",
    "    \n",
    "    beta = elongate_full_beta(beta)\n",
    "    \n",
    "     # Ensure no dividing by zero\n",
    "    for i in range(9):\n",
    "        if Nj[i] == 0:\n",
    "            Nj[i] = 1\n",
    "    \n",
    "    # Set up initial y0 vector\n",
    "    y0 = np.concatenate((S0,I0,I_total0))\n",
    "    sparse_p = csr_matrix(p_matrix)\n",
    "    \n",
    "    beta_p_matrix = sparse_p.multiply(beta[:, None])\n",
    "    \n",
    "    t = np.linspace(0,timesteps,num=timesteps)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def sir(y,timestep):\n",
    "        S = y[:9]\n",
    "        I = y[9:18]\n",
    "        I_total = y[18:]\n",
    "\n",
    "        dSdt = np.zeros_like(S)\n",
    "        dIdt = np.zeros_like(I)\n",
    "        dI_totaldt = np.zeros_like(I_total)\n",
    "\n",
    "        Ij = p_matrix.T @ I\n",
    "        \n",
    "        \n",
    "        method = \"vector\"\n",
    "        \n",
    "        if method == \"vector\":\n",
    "\n",
    "            # Compute Ij_div_Nj efficiently\n",
    "            Ij_div_Nj = Ij / Nj  # Shape (63001,)\n",
    "        \n",
    "            # Vectorized computation of infection terms using sparse matrix multiplication\n",
    "            infection_terms = beta_p_matrix.multiply(S[:, None])  # (63001, 63001)\n",
    "            infection_terms = infection_terms @ Ij_div_Nj  # (63001,)\n",
    "        \n",
    "            # Update derivatives\n",
    "            dSdt -= infection_terms\n",
    "            dIdt += infection_terms\n",
    "            \n",
    "\n",
    "        else:\n",
    "            # Essentially, for all i, all j, dSdt[i] += -beta[j]*p_matrix[i][j]*S[i]*Ij[j]/Nj[j]\n",
    "            for i in valid_patch_indices:\n",
    "                for j in valid_patch_indices:\n",
    "                    dSdt[i] += -beta[j]*p_matrix[i][j]*S[i]*Ij[j]/Nj[j]\n",
    "            dIdt -= dSdt\n",
    "    \n",
    "        dI_totaldt += dIdt\n",
    "\n",
    "        # Add the gamma terms\n",
    "        dIdt -= gamma * I\n",
    "        # dSdt += gamma * I\n",
    "\n",
    "        # Concatenate results\n",
    "        dx = np.concatenate((dSdt, dIdt, dI_totaldt))\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    y_log = np.zeros((len(t)+1,27))\n",
    "    y_log[0] = y0.copy()\n",
    "    \n",
    "\n",
    "    for step in range(len(t)):\n",
    "        results = sir(y_log[step],step)\n",
    "        y_log[step+1] = y_log[step] + results\n",
    "    \n",
    "    solution = process_solution(y_log.T,timesteps)\n",
    "    \n",
    "    end_time = time.time()  # End the timer\n",
    "    elapsed_time = end_time - start_time\n",
    "    # print(f\"run_model_London took {elapsed_time:.4f} seconds\")\n",
    "    \n",
    "    return solution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2199067",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'expand_concise_betas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 183\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m squared_error\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Run the function\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m betas0 \u001b[38;5;241m=\u001b[39m \u001b[43msir_simulation_fit_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_station_timeseries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m betas \u001b[38;5;241m=\u001b[39m elongate_beta(betas0)\n\u001b[1;32m    185\u001b[0m betas\n",
      "Cell \u001b[0;32mIn[22], line 89\u001b[0m, in \u001b[0;36msir_simulation_fit_class\u001b[0;34m(full_timeseries, pop_dict, plot_option)\u001b[0m\n\u001b[1;32m     86\u001b[0m beta_bounds \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.9\u001b[39m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(valid_patch_indices)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Optimize beta with bounds\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_guess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPowell\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta_bounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m beta_opt \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# print('Estimated parameter (beta):', beta_opt)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# print('Final minimized error:', result.fun)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Total population\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_minimize.py:729\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    726\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_neldermead(fun, x0, args, callback, bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[1;32m    727\u001b[0m                                \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpowell\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 729\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_powell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    731\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cg(fun, x0, args, jac, callback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_optimize.py:3518\u001b[0m, in \u001b[0;36m_minimize_powell\u001b[0;34m(func, x0, args, callback, bounds, xtol, ftol, maxiter, maxfev, disp, direc, return_all, **unknown_options)\u001b[0m\n\u001b[1;32m   3514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(lower_bound \u001b[38;5;241m>\u001b[39m x0) \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(x0 \u001b[38;5;241m>\u001b[39m upper_bound):\n\u001b[1;32m   3515\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial guess is not within the specified bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3516\u001b[0m                       OptimizeWarning, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m-> 3518\u001b[0m fval \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3519\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/optimize/_optimize.py:542\u001b[0m, in \u001b[0;36m_wrap_scalar_function_maxfun_validation.<locals>.function_wrapper\u001b[0;34m(x, *wrapper_args)\u001b[0m\n\u001b[1;32m    540\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# A copy of x is sent to the user function (gh13740)\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwrapper_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;66;03m# Ideally, we'd like to a have a true scalar returned from f(x). For\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;66;03m# backwards-compatibility, also allow np.array([1.3]),\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# np.array([[1.3]]) etc.\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[22], line 89\u001b[0m, in \u001b[0;36msir_simulation_fit_class.<locals>.<lambda>\u001b[0;34m(beta)\u001b[0m\n\u001b[1;32m     86\u001b[0m beta_bounds \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.9\u001b[39m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(valid_patch_indices)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Optimize beta with bounds\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m result \u001b[38;5;241m=\u001b[39m minimize(\u001b[38;5;28;01mlambda\u001b[39;00m beta: \u001b[43merror_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpop_dict\u001b[49m\u001b[43m)\u001b[49m, initial_guess, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPowell\u001b[39m\u001b[38;5;124m'\u001b[39m, callback\u001b[38;5;241m=\u001b[39mcallback, bounds\u001b[38;5;241m=\u001b[39mbeta_bounds)\n\u001b[1;32m     94\u001b[0m beta_opt \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# print('Estimated parameter (beta):', beta_opt)\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# print('Final minimized error:', result.fun)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Total population\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 164\u001b[0m, in \u001b[0;36merror_function\u001b[0;34m(beta, data, pop_dict)\u001b[0m\n\u001b[1;32m    160\u001b[0m I_total_0 \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    162\u001b[0m params \u001b[38;5;241m=\u001b[39m [S0,I0,I_total_0,betas,gamma,timespan]\n\u001b[0;32m--> 164\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model_London\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Extract cumulative cases\u001b[39;00m\n\u001b[1;32m    167\u001b[0m I_sum_model \u001b[38;5;241m=\u001b[39m compress_timeseries(y\u001b[38;5;241m.\u001b[39mT)\n",
      "Cell \u001b[0;32mIn[21], line 86\u001b[0m, in \u001b[0;36mrun_model_London\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     82\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     84\u001b[0m S0,I0,I_total0,beta,gamma,timesteps \u001b[38;5;241m=\u001b[39m params\n\u001b[0;32m---> 86\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[43mexpand_concise_betas\u001b[49m(beta)\n\u001b[1;32m     88\u001b[0m  \u001b[38;5;66;03m# Ensure no dividing by zero\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'expand_concise_betas' is not defined"
     ]
    }
   ],
   "source": [
    "def expand_timeseries(short_timeseries):\n",
    "    \n",
    "    transposed = short_timeseries.T\n",
    "    \n",
    "    long_timeseries = np.zeros((14,9))\n",
    "    for i in range(14):\n",
    "        long_timeseries[i] = expand_array(transposed[i])\n",
    "        \n",
    "    long_timeseries = np.array(long_timeseries)\n",
    "    \n",
    "    return long_timeseries\n",
    "\n",
    "def expand_array(short_array):\n",
    "    long_array = np.zeros(9)  # Create a zero array of the desired size\n",
    "\n",
    "    # Assign values at the correct positions\n",
    "    long_array[np.arange(3) * (3 + 1)] = short_array\n",
    "    return long_array\n",
    "\n",
    "\n",
    "\n",
    "def compress_timeseries(long_timeseries):\n",
    "    compressed = np.zeros((14, 3))  # Adjusted to match the original short array shape\n",
    "    \n",
    "    for i in range(14):\n",
    "        compressed[i] = compress_array(long_timeseries[i])\n",
    "        \n",
    "    return compressed.T  # Transpose back to match original input shape\n",
    "\n",
    "def compress_array(long_array):\n",
    "    return long_array[np.arange(3) * (3 + 1)]  # Extract values from the expanded positions\n",
    "\n",
    "\n",
    "def expand_betas(short_betas,long_length):\n",
    "    \n",
    "    long_betas = np.zeros(long_length)\n",
    "    index = 0\n",
    "    for i in valid_patch_indices:\n",
    "        long_betas[i] = short_betas[index]\n",
    "        index += 1\n",
    "    \n",
    "    return long_betas\n",
    "\n",
    "def elongate_beta(short_beta):\n",
    "    long_beta = np.zeros(9)\n",
    "    index = 0\n",
    "    for i in range(9):\n",
    "        if i in home_patches:\n",
    "            long_beta[i] = short_beta[index]\n",
    "            index += 1\n",
    "        else:\n",
    "            long_beta[i] = short_beta[-1]\n",
    "    return long_beta\n",
    "\n",
    "def elongate_full_beta(short_beta):\n",
    "    long_beta = np.zeros(9)\n",
    "    long_beta[valid_patch_indices] = short_beta\n",
    "    return long_beta\n",
    "\n",
    "\n",
    "def sir_simulation_fit_class(full_timeseries, pop_dict, plot_option):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    working_timeseries = expand_timeseries(full_timeseries)\n",
    "    \n",
    "    short_pop_vector = np.array(list(pop_dict.values()))\n",
    "    pop_vector = expand_array(short_pop_vector)\n",
    "    \n",
    "    day_list = np.arange(14)  # There are 14 days\n",
    "\n",
    "    # Stack so each row is [day_index, corresponding 9 values]\n",
    "    data = np.column_stack((day_list[:, None], working_timeseries))\n",
    "\n",
    "\n",
    "    # Initial guess for beta\n",
    "    # initial_guess = 0.2*np.ones(len(valid_patch_indices)) # Randomly distributed betas\n",
    "    initial_guess = np.random.uniform(0.05, 0.9, len(valid_patch_indices))\n",
    "\n",
    "    \n",
    "    def callback(beta, res=None):  # Add a second optional argument\n",
    "        # print(f'Current best beta: {beta}')\n",
    "        a = 1+1\n",
    "\n",
    "    # Define bounds for beta values\n",
    "    beta_bounds = [(0.05, 0.9)] * len(valid_patch_indices)\n",
    "\n",
    "    # Optimize beta with bounds\n",
    "    result = minimize(lambda beta: error_function(beta, data, pop_dict), initial_guess, method='Powell', callback=callback, bounds=beta_bounds)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    beta_opt = result.x\n",
    "\n",
    "    # print('Estimated parameter (beta):', beta_opt)\n",
    "    # print('Final minimized error:', result.fun)\n",
    "\n",
    "    # Total population\n",
    "    N = pop_vector.copy()\n",
    "\n",
    "    # Time span\n",
    "    tspan = np.linspace(0, 14, 14)\n",
    "    \n",
    "    transit_multiplier = 1\n",
    "    gamma = 1 / 5  # Fixed gamma value\n",
    "    betas = beta_opt\n",
    "    I0 = data[0][1:]\n",
    "    S0 = pop_vector-data[0][1:]\n",
    "    I_total_0 = data[0][1:]\n",
    "    \n",
    "    params = [S0,I0,I_total_0,betas,gamma,int(np.max(data[:, 0]))]\n",
    "\n",
    "    # Solve SIR model\n",
    "\n",
    "    y = run_model_London(params)[2]\n",
    "\n",
    "    # Extract cumulative cases\n",
    "    I_sum_model = compress_timeseries(y.copy().T)\n",
    "\n",
    "\n",
    "    if plot_option == \"plot\":\n",
    "        # Plot results\n",
    "        for i in range(3):\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(tspan, I_sum_model[i], 'r-', linewidth=2, label='Model Cumulative Cases')\n",
    "            plt.scatter(data[:, 0], data[:, 1], c='k', s=50, label='Observed Cumulative Cases')\n",
    "\n",
    "            plt.xlabel('Time (Days)')\n",
    "            plt.ylabel('Number of Individuals')\n",
    "            plt.title(f'Fitting for station {station_list[i]}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    # print(\"Total time:\",end_time-start_time)\n",
    "    \n",
    "    return beta_opt\n",
    "\n",
    "\n",
    "def error_function(beta, data, pop_dict):\n",
    "    short_pop_vector = np.array(list(pop_dict.values()))\n",
    "    pop_vector = np.zeros(9)  # Create a zero array of the desired size\n",
    "\n",
    "    # Assign values at the correct positions\n",
    "    pop_vector[np.arange(3) * (3 + 1)] = short_pop_vector\n",
    "    \n",
    "    betas = beta.copy()\n",
    "\n",
    "    tspan = data[:, 0]  # Use observed time points\n",
    "    timespan = tspan.shape[0]-1\n",
    "\n",
    "    # Solve SIR model with cumulative cases\n",
    "    gamma = 1 / 5  # Fixed gamma value\n",
    "    \n",
    "        \n",
    "    I0 = data[0][1:]\n",
    "    S0 = pop_vector-I0\n",
    "    I_total_0 = data[0][1:]\n",
    "    \n",
    "    params = [S0,I0,I_total_0,betas,gamma,timespan]\n",
    "    \n",
    "    y = run_model_London(params)[2]\n",
    "\n",
    "    # Extract cumulative cases\n",
    "    I_sum_model = compress_timeseries(y.T)\n",
    "    raw_real_data = compress_timeseries(data.T[1:].T)\n",
    "    normalizer = compress_array(pop_vector)[:, np.newaxis]\n",
    "    test_data = I_sum_model/normalizer\n",
    "    real_data = raw_real_data/normalizer\n",
    "\n",
    "    # Compute squared error between observed and modeled cumulative cases\n",
    "    squared_error = np.sum((test_data - real_data) ** 2)\n",
    "    \n",
    "    lambda_reg = 1e-6\n",
    "    penalty = lambda_reg * np.sum((beta - np.mean(beta))**2)\n",
    "    squared_error += penalty\n",
    "\n",
    "    return squared_error\n",
    "\n",
    "# Run the function\n",
    "betas0 = sir_simulation_fit_class(all_station_timeseries, pop_dict, \"plot\")\n",
    "betas = elongate_beta(betas0)\n",
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cca96588",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_patches = np.array([0,4,8])\n",
    "travel_patches = np.array([1,3,5,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f266e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35760972, 0.46895792, 0.        , 0.4691559 , 0.70063527,\n",
       "       0.46925021, 0.        , 0.46933183, 0.35037139])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elongate_full_beta(betas0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b49e85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35760972, 0.46895792, 0.4691559 , 0.70063527, 0.46925021,\n",
       "       0.46933183, 0.35037139])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc3f1736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.4693303189431857)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(betas0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "722b6c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.46917396262494465)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(elongate_full_beta(betas0)[travel_patches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ae2f8e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.30670458, 1.12090707, 0.        , 1.12091368, 0.35739214,\n",
       "        0.43169611, 0.        , 0.4316936 , 0.1669445 ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.39313136, 0.79264143, 0.        , 0.7926461 , 2.7545871 ,\n",
       "        0.69071377, 0.        , 0.69070975, 0.34767083],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.2003334 , 0.48038875, 0.        , 0.48039158, 0.37927727,\n",
       "        1.27154126, 0.        , 1.27153387, 1.28898897]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.ones(9)*(1/5)\n",
    "\n",
    "\n",
    "def making_NGM(p_matrix,N_array,betas,alphas):\n",
    "    size = betas.shape[0]  # 6084 for full London system\n",
    "    print(size)\n",
    "    NGM = np.zeros([size,size])\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            N_div_a = N_array[i]/alphas[j]\n",
    "            k_sum = 0\n",
    "            for k in range(size):\n",
    "                l_sum = 0\n",
    "                for l in range(size):\n",
    "                    l_sum += N_array[l]*p_matrix[l][k]\n",
    "                if l_sum != 0:\n",
    "                    k_sum += betas[k]*p_matrix[i][k]*p_matrix[j][k]/l_sum\n",
    "            NGM[i][j] = k_sum*N_div_a\n",
    "    return NGM\n",
    "\n",
    "NGM1 = making_NGM(p_matrix,pop_vector,betas,alphas)\n",
    "NGM1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f8e0a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing l_sums...\n",
      "Computing k_sums...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing k_sums: 100%|██████████| 9/9 [00:00<00:00, 827.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing NGM computation...\n",
      "NGM computation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.30670458, 1.12090707, 0.        , 1.12091368, 0.35739214,\n",
       "        0.43169611, 0.        , 0.4316936 , 0.1669445 ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.39313136, 0.79264143, 0.        , 0.7926461 , 2.7545871 ,\n",
       "        0.69071377, 0.        , 0.69070975, 0.34767083],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.2003334 , 0.48038875, 0.        , 0.48039158, 0.37927727,\n",
       "        1.27154126, 0.        , 1.27153387, 1.28898897]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas = np.ones(9)*(1/5)\n",
    "\n",
    "\n",
    "def expand_concise_betas(home_patches,travel_patches,concise_beta):\n",
    "    print(concise_beta.shape)\n",
    "    long_beta = np.zeros(9)\n",
    "    long_beta[np.concatenate([home_patches,travel_patches])] = concise_beta\n",
    "    \n",
    "    return long_beta\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "def making_NGM(sparse_p, N_array, betas, alphas):\n",
    "    size = betas.shape[0]  \n",
    "\n",
    "    print(\"Computing l_sums...\")\n",
    "    l_sums = sparse_p.T.dot(N_array)  # Efficient matrix-vector multiplication\n",
    "    l_sums[l_sums == 0] = 1  # Avoid division by zero\n",
    "\n",
    "    print(\"Computing k_sums...\")\n",
    "    k_sums = csr_matrix((size, size))  # Initialize sparse result matrix\n",
    "\n",
    "    # Iterate over nonzero elements of sparse_p\n",
    "    for k in tqdm(range(size), desc=\"Processing k_sums\"):\n",
    "        if l_sums[k] == 0:  # Skip empty columns\n",
    "            continue\n",
    "        k_sums += sparse_p[:, k].dot((betas[k] * sparse_p[:, k].T / l_sums[k]))\n",
    "\n",
    "    print(\"Finalizing NGM computation...\")\n",
    "    NGM = (N_array[:, None] / alphas) * k_sums.toarray()  # Convert to dense if necessary\n",
    "\n",
    "    print(\"NGM computation complete.\")\n",
    "    return NGM\n",
    "\n",
    "NGM2 = making_NGM(sparse_p,pop_vector,betas,alphas)\n",
    "NGM2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "459d4a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Top Eigenvalue: [2.94111407+0.j]\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "# Ensure NGM is stored efficiently in CSR format\n",
    "NGM_sparse = csr_matrix(NGM2)\n",
    "\n",
    "# Compute only the largest eigenvalue using shift-invert mode\n",
    "eigenvalues, eigenvectors = eigs(NGM_sparse, k=1, which=\"LM\", tol=1e-6)\n",
    "\n",
    "print(\"Computed Top Eigenvalue:\", eigenvalues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593869d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b9c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049e304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3045f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e3b0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b70c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77405c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab521194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c9f7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized beta: [0.34393933 0.47945321 0.4794389  0.7712417  0.47944539 0.47948074\n",
      " 0.32118925]\n",
      "Final minimized error: 0.0030887703219116885\n",
      "Total time: 0.720421314239502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.34393933, 0.47945321, 0.4794389 , 0.7712417 , 0.47944539,\n",
       "       0.47948074, 0.32118925])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parallel beta fitting\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def error_function(beta, data, pop_dict):\n",
    "    \"\"\"Compute error in parallel using multiple beta evaluations.\"\"\"\n",
    "    \n",
    "    # Expand population vector\n",
    "    short_pop_vector = np.array(list(pop_dict.values()))\n",
    "    pop_vector = expand_array(short_pop_vector)\n",
    "\n",
    "    # Initial conditions\n",
    "    I0 = data[0][1:]\n",
    "    S0 = pop_vector - I0\n",
    "    I_total_0 = np.zeros(9)\n",
    "    gamma = 1 / 5\n",
    "    timespan = data.shape[0] - 1\n",
    "\n",
    "    params = [S0, I0, I_total_0, beta, gamma, timespan]\n",
    "    \n",
    "    # Run SIR model simulations in parallel\n",
    "    y_list = Parallel()(\n",
    "        delayed(run_model_London)(params) for _ in range(len(valid_patch_indices))\n",
    "    )\n",
    "\n",
    "    # Extract cumulative cases\n",
    "    I_sum_model = compress_timeseries(y_list[0][2].T)\n",
    "    raw_real_data = compress_timeseries(data.T[1:].T)\n",
    "    \n",
    "    # Normalize\n",
    "    normalizer = compress_array(pop_vector)[:, np.newaxis]\n",
    "    test_data = I_sum_model / normalizer\n",
    "    real_data = raw_real_data / normalizer\n",
    "\n",
    "    # Compute squared error\n",
    "    squared_error = np.sum((test_data - real_data) ** 2)\n",
    "    \n",
    "    # Regularization penalty\n",
    "    penalty = 1e-3 * np.sum((beta - np.mean(beta)) ** 2)\n",
    "    \n",
    "    return squared_error + penalty\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def sir_simulation_fit_class(full_timeseries, pop_dict, plot_option):\n",
    "    \"\"\"Fit the SIR model parameters using parallelized optimization.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Expand time series\n",
    "    working_timeseries = expand_timeseries(full_timeseries)\n",
    "    \n",
    "    # Set up data for fitting\n",
    "    day_list = np.arange(14)  \n",
    "    data = np.column_stack((day_list[:, None], working_timeseries))\n",
    "\n",
    "    # Initial guess for beta\n",
    "    initial_guess = np.random.uniform(0.2, 0.6, len(valid_patch_indices))\n",
    "\n",
    "    # Define bounds for beta values\n",
    "    beta_bounds = [(0.05, 0.9)] * len(valid_patch_indices)\n",
    "\n",
    "    # Optimize beta with parallel error function\n",
    "    result = minimize(\n",
    "        lambda beta: error_function(beta, data, pop_dict), \n",
    "        initial_guess, \n",
    "        method='trust-constr',  \n",
    "        bounds=beta_bounds,\n",
    "        options={'maxiter': 1000, 'gtol': 1e-6}\n",
    "    )\n",
    "\n",
    "    beta_opt = result.x\n",
    "\n",
    "    print('Optimized beta:', beta_opt)\n",
    "    print('Final minimized error:', result.fun)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(\"Total time:\",end_time-start_time)\n",
    "\n",
    "    return beta_opt\n",
    "\n",
    "sir_simulation_fit_class(all_station_timeseries, pop_dict, \"plot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
